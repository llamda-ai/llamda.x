File list
*********


llamda.py
=========

Main entry point for the library.


llamda_fn/llms
==============

Files for working with LLMs.


api_types.py
------------

from ast import Dict import uuid from functools import cached_property
from typing import Any, Literal, Self, List

from openai.types.chat import ChatCompletion as OaiCompletion from
openai.types.chat import ChatCompletionToolParam as OaiToolParam from
openai.types.chat import ChatCompletionMessageParam as
OaiRequestMessage from openai.types.chat import
ChatCompletionAssistantMessageParam as OaiAssistantMessage from
openai.types.chat import ChatCompletionUserMessageParam as
OaiUserMessage from openai.types.chat import
ChatCompletionSystemMessageParam as OaiSystemMessage from
openai.types.chat import ChatCompletionMessageToolCall as OaiToolCall
from openai.types.chat import ChatCompletionFunctionCallOptionParam as
OaiToolFunction from pydantic import BaseModel, Field

Role = Literal["user", "system", "assistant", "tool"]

class LlToolCall(BaseModel):
   """ Describes a function call from the LLM """

   id: str name: str arguments: str

   @classmethod def from_oai_tool_call(cls, call: OaiToolCall) ->
   Self:

      """Gets data from the Openai Tool Call""" return cls(

         id=call.id, name=call.function.name,
         arguments=call.function.arguments,

      )

class ToolResponse(BaseModel):
   id: str _result: str

   def __init__(self, result: str = "", >>**<<kwargs: Any) -> None:
      super().__init__(>>**<<kwargs) self._result = result

   @cached_property def result(self) -> str:

      if isinstance(self._result, BaseModel):
         return self._result.model_dump_json()

      else:
         return self._result

def make_oai_role_message(
   role: Role, content: str, name: str | None = None, tool_calls:
   List[LlToolCall] | None = None, >>**<<kwargs: Any,

) -> OaiRequestMessage:
   message = {
      "role": role, "content": content,

   } if name:

      message["name"] = name

   return message

OaiRoleMessage: dict[
   Role, type[OaiUserMessage] | type[OaiSystemMessage] |
   type[OaiAssistantMessage]

] = {
   "user": OaiUserMessage, "system": OaiSystemMessage, "assistant":
   OaiAssistantMessage,

}

class LLMessageMeta(BaseModel):
   choice: dict[str, Any] | None = Field(exclude=True) completion:
   dict[str, Any] | None = Field(exclude=True)

class LLMessage(BaseModel):
   id: str = Field(default_factory=uuid.uuid4) role: Role = "user"
   content: str name: str | None = None tool_calls: List[LlToolCall] |
   None = None meta: LLMessageMeta | None = None

   def get_oai_message(self):
      return {
         "role": self.role, "content": self.content, "name":
         self.name, "tool_calls": (

            [tool_call.model_dump() for tool_call in
            [>>*<<self.tool_calls]] if self.tool_calls else None

         ),

      }

   @classmethod def from_execution(cls, execution: ToolResponse) ->
   Self:

      return cls(
         role="tool", id=execution.id, content=execution.result,

      )

class LLCompletion(BaseModel):
   message: LLMessage meta: LLMessageMeta | None = None

   @classmethod def from_completion(cls, completion: OaiCompletion) ->
   Self:

      choice = completion.choices[0] message = choice.message
      tool_calls = None if message.tool_calls:

         tool_calls = [
            LlToolCall.from_oai_tool_call(tc) for tc in
            message.tool_calls

         ]

      return cls(
         message=LLMessage(
            id=completion.id, meta=LLMessageMeta(

               choice=choice.model_dump(exclude={"message"}),
               completion=completion.model_dump(exclude={"choices"}),

            ), role=message.role, content=message.content or "",
            tool_calls=tool_calls,

         )

      )

class OaiRequest(BaseModel):
   messages: list[OaiRequestMessage] tools: list[OaiToolParam]

__all__ = [
   "LLMessage", "LLCompletion", "OaiCompletion", "OaiToolParam",
   "OaiToolFunction", "OaiToolCall",

]


api.py
------

"""Module to handle the LLM APIs."""

from os import environ from typing import Any, Optional import dotenv

from pydantic import BaseModel, Field, field_validator from openai
import OpenAI

dotenv.load_dotenv()

class LlmApiConfig(BaseModel):
   """ Configuration for the LLM API. """

   base_url: Optional[str] = None api_key: Optional[str] = Field(

      exclude=True, alias="api_key",
      default=environ.get("OPENAI_API_KEY"),

   ) organization: Optional[str] = None timeout: Optional[float] =
   None max_retries: Optional[int] = None default_headers:
   Optional[dict[str, Any]] = None default_query: Optional[dict[str,
   Any]] = None http_client: Optional[Any] = None  # You might want to
   use a more specific type here

   @field_validator("api_key") @classmethod def validate_api_key(cls,
   v: Optional[str], info: Any) -> Optional[str]:

      """ Validate the API key. """ if not v and "base_url" not in
      info.data:

         raise ValueError("API key is required when base_url is not
         provided")

      return v

   def create_openai_client(self) -> OpenAI:
      """ Create and return an OpenAI client with the configured
      settings. """ config = {k: v for k, v in
      self.model_dump().items() if v is not None} return
      OpenAI(>>**<<config)

__all__: list[str] = [
   "LlmApiConfig",

]


llm_manager.py
--------------

from typing import Any from pydantic import Field, model_validator
from openai import OpenAI from openai.types.chat import ChatCompletion

from llamda_fn.llms.exchange import Exchange from .api_types import
LLCompletion from .api import LlmApiConfig

class LLManager(OpenAI):
   api_config: dict[str, Any] = Field(default_factory=dict) llm_name:
   str = Field(default="gpt-4-0613")

   def __init__(
      self, llm_name: str = "gpt-4-0613", >>**<<kwargs: Any,

   ):
      self.llm_name = llm_name super().__init__(>>**<<kwargs)

   class Config:
      arbitrary_types_allowed = True

   def chat_completion(
      self, messages: Exchange, llm_name: str, >>**<<kwargs: Any

   ) -> LLCompletion:
      oai_messages = [] for message in messages:

         oai_message = message.get_oai_message() oai_messages.append(

            {
               "role": message.role, "content":
               oai_message.get("content"),

            }

         )

      try:
         oai_completion: ChatCompletion =
         self.chat.completions.create(
            messages=oai_messages, model=llm_name or self.llm_name,
            >>**<<kwargs,

         ) return LLCompletion.from_completion(oai_completion)

      except Exception as e:
         raise Exception(e, messages)

   @model_validator(mode="before") @classmethod def
   validate_api_and_model(cls, data: dict[str, Any]) -> dict[str,
   Any]:

      """Validate the API and model.""" api_config =
      data.get("api_config") or {} api = (

         data.get("api") if isinstance(data.get("api"), OpenAI) else
         LlmApiConfig(>>**<<api_config).create_openai_client()

      ) if not api or not isinstance(api, OpenAI):

         raise ValueError("Unable to create OpenAI client.")

      data.update({"api": api})

      if data.get("llm_name"):
         available_models: list[str] = [model.id for model in
         api.models.list()] if data.get("llm_name") not in
         available_models:

            raise ValueError(
               f"Model '{data.get('llm_name')}' is not available. "
               f"Available models: {', '.join(available_models)}"

            )

      else:
         raise ValueError("No LLM API client or LLM name provided.")

      return data


type_transformers.py
--------------------

from typing import Any, List from llamda_fn.llms.api_types import (

   Role, LlToolCall, OaiUserMessage, OaiSystemMessage,
   OaiAssistantMessage,

)

def make_oai_message(
   role: Role, content: str, name: str | None = None, tool_calls:
   List[LlToolCall] | None = None, >>**<<kwargs: Any,

) -> OaiUserMessage | OaiSystemMessage | OaiAssistantMessage:
   kwargs = {} if name:

      kwargs["name"] = name

   match role:
      case "user":
         return OaiUserMessage(
            content=content, >>**<<kwargs,

         )

      case "system":
         return OaiSystemMessage(
            content=content, >>**<<kwargs,

         )

      case "assistant":

         if tool_calls:
            kwargs["tool_calls"] = [
               tool_call.model_dump() for tool_call in tool_calls

            ]

         return OaiAssistantMessage(
            content=content, >>**<<kwargs,

         )

      case _:
         raise ValueError(f"Invalid role: {role}")

OaiRoleMessage: dict[
   Role, type[OaiUserMessage] | type[OaiSystemMessage] |
   type[OaiAssistantMessage]

] = {
   "user": OaiUserMessage, "system": OaiSystemMessage, "assistant":
   OaiAssistantMessage,

}


exchange.py
-----------

from llamda_fn.llms.api_types import LLMessage

from collections import UserList from typing import List, Optional

class Exchange(UserList[LLMessage]):
   """ An exchange represents a series of messages between a user and
   an assistant. """

   def __init__(
      self, system_message: Optional[str] = None, messages:
      Optional[List[LLMessage]] = None,

   ) -> None:
      super().__init__() if system_message:

         self.data.append(LLMessage(content=system_message,
         role="system"))

      if messages:
         for message in messages:
            if not message.role:
               raise ValueError(f"Message missing role: {message}")

         self.data.extend(messages)

   def ask(self, content: str) -> None:
      self.data.append(LLMessage(content=content, role="user"))

   def append(self, item: LLMessage) -> None:
      """ Add a message to the exchange. """

      self.data.append(item)

   def get_context(self, n: int = 5) -> list[LLMessage]:
      """ Get the last n messages as context. """ return
      self.data[-n:]

   def __str__(self) -> str:
      """ String representation of the exchange. """ return
      "n".join(f"{msg.role}: {msg.content}" for msg in self.data)


llamda_fn/functions
===================


llamda_classes.py
-----------------

from typing import Any, Callable, Dict, Generic, TypeVar, Type from
pydantic import BaseModel, Field, create_model, ConfigDict

from llamda_fn.llms.api_types import OaiToolParam

R = TypeVar("R")

class LlamdaCallable(Generic[R]):
   def run(self, >>**<<kwargs: Any) -> R:
      raise NotImplementedError

   def to_tool_schema(self) -> OaiToolParam:
      raise NotImplementedError

   @classmethod def create(

      cls, call_func: Callable[..., R], name: str = "", description:
      str = "", >>**<<kwargs: Any,

   ) -> "LlamdaCallable[R]":
      raise NotImplementedError

class LlamdaBase(BaseModel, LlamdaCallable[R]):
   """The base class for Llamda functions."""

   name: str description: str call_func: Callable[..., R]

   model_config = ConfigDict(arbitrary_types_allowed=True)

   def to_schema(self) -> Dict[str, Any]:
      """Get the JSON schema for the Llamda function.""" raise
      NotImplementedError

   def to_tool_schema(self) -> OaiToolParam:
      """Get the JSON schema for the LlamdaPydantic.""" schema =
      self.to_schema() return {

         "type": "function", "function": {

            "name": schema["title"], "description":
            schema["description"], "parameters": {

               "type": "object", "properties": schema["properties"],
               "required": schema.get("required", []),

            },

         },

      }

class LlamdaFunction(LlamdaBase[R]):
   """A Llamda function that uses a simple function model as the
   input."""

   parameter_model: Type[BaseModel]

   @classmethod def create(

      cls, call_func: Callable[..., R], name: str = "", description:
      str = "", fields: Dict[str, tuple[type, Any]] = {},
      >>**<<kwargs: Any,

   ) -> "LlamdaFunction[R]":
      """Create a new LlamdaFunction from a function.""" model_fields
      = {} for field_name, (field_type, field_default) in
      fields.items():

         print(field_name, field_default, field_type) if field_default
         is ...:

            model_fields[field_name] = (field_type, Field(...))

         else:
            model_fields[field_name] = (field_type,
            Field(default=field_default))

      parameter_model: type[BaseModel] = create_model(
         f"{name}Parameters", >>**<<model_fields

      )

      return cls(
         name=name, description=description,
         parameter_model=parameter_model, call_func=call_func,

      )

   def run(self, >>**<<kwargs: Any) -> R:
      """Run the LlamdaFunction with the given parameters."""
      validated_params = self.parameter_model(>>**<<kwargs) return
      self.call_func(>>**<<validated_params.model_dump())

   def to_schema(self) -> Dict[str, Any]:
      """Get the JSON schema for the LlamdaFunction.""" schema =
      self.parameter_model.model_json_schema() schema["title"] =
      self.name schema["description"] = self.description return schema

class LlamdaPydantic(LlamdaBase[R]):
   """A Llamda function that uses a Pydantic model as the input."""

   model: Type[BaseModel]

   @classmethod def create(

      cls, call_func: Callable[..., R], name: str = "", description:
      str = "", model: Type[BaseModel] = BaseModel, >>**<<kwargs: Any,

   ) -> "LlamdaPydantic[R]":
      """Create a new LlamdaPydantic from a Pydantic model."""

      return cls(
         name=name, description=description, call_func=call_func,
         model=model,

      )

   def run(self, >>**<<kwargs: Any) -> R:
      """Run the LlamdaPydantic with the given parameters."""
      validated_params = self.model(>>**<<kwargs) return
      self.call_func(validated_params)

   def to_schema(self) -> dict[str, Any]:
      """Get the JSON schema for the LlamdaPydantic.""" schema:
      dict[str, Any] =
      self.model.model_json_schema(mode="serialization")
      schema["title"] = self.name schema["description"] =
      self.description return schema


process_fields.py
-----------------

from ast import List from typing import Any, Dict, Union, get_args,
get_origin from pydantic import BaseModel, Field, ValidationError,
create_model from pydantic.fields import FieldInfo from pydantic_core
import SchemaError

JsonDict = Dict[str, Any]

def process_field(
   field_type: Any, field_info: Union[JsonDict, FieldInfo]

) -> tuple[Any, JsonDict]:
   """ Process a field type and info, using Pydantic's
   model_json_schema for schema generation. """ try:

      if isinstance(field_type, type) and issubclass(field_type,
      BaseModel):
         # Handle nested Pydantic models nested_schema =
         field_type.model_json_schema() field_schema = {

            "type": "object", "properties":
            nested_schema.get("properties", {}), "required":
            nested_schema.get("required", []),

         }

      else:
         # Create a temporary model with the field if
         isinstance(field_info, FieldInfo):

            temp_field = field_info

         else:
            temp_field = Field(>>**<<field_info)

         TempModel = create_model("TempModel", field=(field_type,
         temp_field))

         # Get the JSON schema for the entire model full_schema =
         TempModel.model_json_schema()

         # Extract the schema for our specific field field_schema =
         full_schema["properties"]["field"]

      # Handle Optional types origin = get_origin(field_type) if
      origin is Union:

         args = get_args(field_type) if type(None) in args:

            # This is an Optional type non_none_type = next(arg for
            arg in args if arg is not type(None)) if non_none_type is
            float:

               field_schema = {"type": "number", "nullable": True}

            elif non_none_type is int:
               field_schema = {"type": "integer", "nullable": True}

            elif non_none_type is str:
               field_schema = {"type": "string", "nullable": True}

            elif isinstance(non_none_type, type) and issubclass(
               non_none_type, BaseModel

            ):
               field_schema = {"type": "object", "nullable": True}

      # Ensure 'type' is always set if "type" not in field_schema:

         if isinstance(field_type, type) and issubclass(field_type,
         BaseModel):
            field_schema["type"] = "object"

         elif field_type is int:
            field_schema["type"] = "integer"

         elif field_type is float:
            field_schema["type"] = "number"

         elif field_type is str:
            field_schema["type"] = "string"

         elif field_type is bool:
            field_schema["type"] = "boolean"

         elif field_type is list or field_type is List:
            field_schema["type"] = "array"

         elif field_type is dict or field_type is Dict:
            field_schema["type"] = "object"

         else:
            field_schema["type"] = "any"

      # Remove 'title' field if present field_schema.pop("title",
      None)

      # Merge field_info with the generated schema if
      isinstance(field_info, dict):

         for key, value in field_info.items():
            if key not in field_schema or field_schema[key] is None:
               field_schema[key] = value

      return field_type, field_schema

   except (SchemaError, ValidationError) as e:
      print(f"Error processing field: {e}") return Any, {"type":
      "any", "error": str(e)}

def process_fields(fields: Dict[str, Any]) -> Dict[str, tuple[Any,
JsonDict]]:
   """ Process all fields in a model, using Pydantic for complex
   types. """ processed_fields = {} for field_name, field_value in
   fields.items():

      if isinstance(field_value, FieldInfo):
         field_type = field_value.annotation field_info = field_value

      elif isinstance(field_value, tuple):
         field_type, field_info = field_value

      else:
         raise ValueError(
            f"Unexpected field value type for {field_name}:
            {type(field_value)}"

         )

      processed_type, processed_info = process_field(field_type,
      field_info)

      # Ensure 'type' is set for nested Pydantic models if
      isinstance(processed_type, type) and issubclass(processed_type,
      BaseModel):

         processed_info["type"] = "object"

      processed_fields[field_name] = (processed_type, processed_info)

   return processed_fields


process_functions.py
--------------------

import json from inspect import Parameter, isclass, signature from
typing import (

   Any, Callable, Dict, List, Optional, TypeVar, ParamSpec, Sequence,
   Iterator,

)

from pydantic import BaseModel, ValidationError from
llamda_fn.llms.api_types import LlToolCall, ToolResponse, OaiToolParam
from .llamda_classes import LlamdaFunction, LlamdaPydantic,
LlamdaCallable

R = TypeVar("R") P = ParamSpec("P")

class LlamdaFunctions:
   def __init__(self) -> None:
      self._tools: Dict[str, LlamdaCallable[Any]] = {}

   @property def tools(self) -> Dict[str, LlamdaCallable[Any]]:

      return self._tools

   def llamdafy(
      self, name: Optional[str] = None, description: Optional[str] =
      None,

   ) -> Callable[[Callable[P, R]], LlamdaCallable[R]]:
      def decorator(func: Callable[P, R]) -> LlamdaCallable[R]:
         func_name: str = name or func.__name__ func_description: str
         = description or func.__doc__ or ""

         sig = signature(func) if len(sig.parameters) == 1:

            param = next(iter(sig.parameters.values())) if
            isclass(param.annotation) and issubclass(

               param.annotation, BaseModel

            ):
               llamda_func = LlamdaPydantic.create(
                  call_func=func, name=func_name,
                  description=func_description,
                  model=param.annotation,

               ) self._tools[func_name] = llamda_func return
               llamda_func

         fields: Dict[str, tuple[type, Any]] = {
            param_name: (
               param.annotation if param.annotation != Parameter.empty
               else Any, param.default if param.default !=
               Parameter.empty else ...,

            ) for param_name, param in sig.parameters.items()

         }

         llamda_func: LlamdaCallable[R] = LlamdaFunction.create(
            call_func=func, fields=fields, name=func_name,
            description=func_description,

         ) self._tools[func_name] = llamda_func return llamda_func

      return decorator

   def get(self, names: Optional[List[str]] = None) ->
   Sequence[OaiToolParam]:
      """Returns the tool spec for some or all of the functions in the
      registry""" if names is None:

         names = list(self._tools.keys())

      return [
         self._tools[name].to_tool_schema() for name in names if name
         in self._tools

      ]

   def execute_function(self, tool_call: LlToolCall) -> ToolResponse:
      """Executes the function specified in the tool call with the
      required arguments""" try:

         if tool_call.name not in self._tools:
            raise KeyError(f"Function '{tool_call.name}' not found")

         parsed_args = json.loads(tool_call.arguments) result =
         self._tools[tool_call.name].run(>>**<<parsed_args)

      except KeyError as e:
         result = {"error": f"Error: {str(e)}"}

      except ValidationError as e:
         result = {"error": f"Error: Validation failed - {str(e)}"}

      except Exception as e:
         result = {"error": f"Error: {str(e)}"}

      return ToolResponse(
         id=tool_call.id, name=tool_call.name,
         arguments=tool_call.arguments, result=json.dumps(result),

      )

   def __getitem__(self, key: str) -> LlamdaCallable[Any]:
      return self._tools[key]

   def __contains__(self, key: str) -> bool:
      return key in self._tools

   def __len__(self) -> int:
      return len(self._tools)

   def __iter__(self) -> Iterator[str]:
      return iter(self._tools)


examples
========

from io import TextIOWrapper from typing import List, Tuple from
llamda_fn import Llamda import subprocess import traceback

from .functions.simple_function_aq import aq

global ll

ll = Llamda(
   system_message="""You are a cabalistic assistant who is eager to
   help users find weird numerical correspondences between strings.
   """

)

try:

   @ll.fy() def aq_multiple(input_strings: List[str]) ->
   List[Tuple[str, int]]:

      """ Calculate the Alphanumeric Quabala (AQ) value for multiple
      strings.

      This function calculates the AQ value for each string in the
      input list and returns a sorted list of tuples containing the
      original string and its AQ value.

      Args:
         input_strings (List[str]): A list of strings to calculate AQ
         values for.

      Returns:
         List[Tuple[str, int]]: A list of tuples (original_string,
         aq_value) sorted by AQ value.

      """ return sorted([(s, aq(s)) for s in input_strings],
      key=lambda x: x[1])

   ll.send_message("hello")

except Exception as e:
   with open("./OUT", "w", encoding="utf-8") as f:
      f.write(f"Exception: {str(e)}nn") f.write("Traceback:n")
      f.write(traceback.format_exc())

   print(e)

   # Run shell script subprocess.run(["bash", "./scripts/claude.sh"],
   check=True) exit()


latest run
----------

Exception: (BadRequestError('Error code: 400 - {'error': {'message':
"Missing required parameter: 'messages[0].role'.", 'type':
'invalid_request_error', 'param': 'messages[0].role', 'code':
'missing_required_parameter'}}'), [LLMessage(id=UUID('874e0e5e-65ba-
49e5-b3c1-465593ef6cae'), role='system', content='You are a cabalistic
assistant who is eager to help usersn    find weird numerical
correspondences between strings.n    ', name=None, tool_calls=None,
meta=None), LLMessage(id=UUID('a0375520-52f1-41c7-a1a9-a6ac752e1f8c'),
role='user', content='hello', name=None, tool_calls=None, meta=None)])

Traceback: Traceback (most recent call last):

   File "/Users/claudiobrandolino/llamda/llamda-
   fn/llamda_fn/llms/llm_manager.py", line 38, in chat_completion
      oai_completion: ChatCompletion = self.chat.completions.create(
   File "/Users/claudiobrandolino/Library/Caches/pypoetry/virtualenvs
   /llamda-T-M8I_mI-py3.12/lib/python3.12/site-
   packages/openai/_utils/_utils.py", line 275, in wrapper
      return func(>>*<<args, >>**<<kwargs)
   File "/Users/claudiobrandolino/Library/Caches/pypoetry/virtualenvs
   /llamda-T-M8I_mI-py3.12/lib/python3.12/site-
   packages/openai/resources/chat/completions.py", line 663, in create
      return self._post(
   File "/Users/claudiobrandolino/Library/Caches/pypoetry/virtualenvs
   /llamda-T-M8I_mI-py3.12/lib/python3.12/site-
   packages/openai/_base_client.py", line 1200, in post
      return cast(ResponseT, self.request(cast_to, opts,
      stream=stream, stream_cls=stream_cls))
   File "/Users/claudiobrandolino/Library/Caches/pypoetry/virtualenvs
   /llamda-T-M8I_mI-py3.12/lib/python3.12/site-
   packages/openai/_base_client.py", line 889, in request
      return self._request(
   File "/Users/claudiobrandolino/Library/Caches/pypoetry/virtualenvs
   /llamda-T-M8I_mI-py3.12/lib/python3.12/site-
   packages/openai/_base_client.py", line 980, in _request
      raise self._make_status_error_from_response(err.response) from
      None

openai.BadRequestError: Error code: 400 - {'error': {'message':
"Missing required parameter: 'messages[0].role'.", 'type':
'invalid_request_error', 'param': 'messages[0].role', 'code':
'missing_required_parameter'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
   File "/Users/claudiobrandolino/llamda/llamda-
   fn/examples/__main__.py", line 37, in <module>
      ll.send_message("hello")

   File "/Users/claudiobrandolino/llamda/llamda-
   fn/llamda_fn/llamda.py", line 90, in send_message
      return self.run()
   File "/Users/claudiobrandolino/llamda/llamda-
   fn/llamda_fn/llamda.py", line 55, in run
      ll_completion: LLCompletion = self.api.chat_completion(
   File "/Users/claudiobrandolino/llamda/llamda-
   fn/llamda_fn/llms/llm_manager.py", line 45, in chat_completion
      raise Exception(e, messages)

Exception: (BadRequestError('Error code: 400 - {'error': {'message':
"Missing required parameter: 'messages[0].role'.", 'type':
'invalid_request_error', 'param': 'messages[0].role', 'code':
'missing_required_parameter'}}'), [LLMessage(id=UUID('874e0e5e-65ba-
49e5-b3c1-465593ef6cae'), role='system', content='You are a cabalistic
assistant who is eager to help usersn    find weird numerical
correspondences between strings.n    ', name=None, tool_calls=None,
meta=None), LLMessage(id=UUID('a0375520-52f1-41c7-a1a9-a6ac752e1f8c'),
role='user', content='hello', name=None, tool_calls=None, meta=None)])
